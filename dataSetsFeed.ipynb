{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "dataSetsFeed.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM40u+/IZu9faakYJXgmNtF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msan222/Response_Bot/blob/main/dataSetsFeed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VmH4wdN0aiq"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "import gdown\n",
        "#!pip install google.colab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Clix8N59cqqL"
      },
      "source": [
        "import os\n",
        "!git clone --quiet https://github.com/pistocop/messaging-chat-parser.git\n",
        "!pip install -q -r messaging-chat-parser/requirements.txt\n",
        "!git clone --quiet https://github.com/pistocop/pistoBot.git\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRSi2GsHxo3w"
      },
      "source": [
        "gdown.download('https://drive.google.com/uc?id=1f4vOzkZfcqzEAo-yujV0KNpGXUsCeQ_W','test.txt',quiet=False)\n",
        "\n",
        "#https://drive.google.com/file/d/1f4vOzkZfcqzEAo-yujV0KNpGXUsCeQ_W/view?usp=sharing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u87s9jzOzVTa"
      },
      "source": [
        "!mv test.txt ./messaging-chat-parser/data/chat_raw/whatsapp"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHSOoxejg8s4"
      },
      "source": [
        "whatsapp_user_name = \"Maddie_22\" # <--- your name, extracted from Whatsapp data\n",
        "whatsapp_datetime_format = \"%m/%d/%y, %H:%M %p\" # <-- American format used (MDY)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3_N2WUso_XI"
      },
      "source": [
        "# Whatsapp\n",
        "!pwd\n",
        "!echo $path\n",
        "print(\"> [WHATSAPP] start parsing...\")\n",
        "assert whatsapp_user_name is not None, \"[!] Whatsapp user name not setted\"\n",
        "!cd messaging-chat-parser/ && python ./src/whatsapp_parser.py --session_token \"<|endoftext|>\" --user_name $whatsapp_user_name --time_format \"$whatsapp_datetime_format\"\n",
        "print(\"> [WHATSAPP] parsing completed!\\n\\n\")\n",
        "print(\"----------------------------------\")\n",
        "\n",
        "# Join Telegram and Whatsapp data\n",
        "!cd messaging-chat-parser && python ./src/joiner.py\n",
        "training_data_lines = sum(1 for line in open('./messaging-chat-parser/data/chat_parsed/all-messages.txt'))\n",
        "print(f\"> [PARSER] Training file lines: {training_data_lines}\")\n",
        "print(\"----------------------------------\")\n",
        "\n",
        "# Check data size\n",
        "if training_data_lines < 1000:\n",
        "    print(f\"[WARNING] attention insufficient training data ({training_data_lines} < 100K), it is recommended to export more chats\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ipt9F7lGWj7s"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"imdb\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "!pwd\n",
        "from torch.utils.data import Dataset\n",
        "class CustomTextDataset(Dataset):\n",
        "  def __init__(self, file):\n",
        "    with open(file,'r') as infile:\n",
        "      self.text = [line for line in infile]\n",
        "        \n",
        "  def __len__(self):\n",
        "    return len(self.text)\n",
        "\n",
        "  def __getitem__(self, idx):        \n",
        "    return {'text':self.text[idx], 'label': 0}\n",
        "\n",
        "\n",
        "r_j_dataset = CustomTextDataset('./messaging-chat-parser/data/chat_parsed/user-messages.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kknw7liKnxLr",
        "outputId": "e20fe9c6-7415-453d-c4f2-641c01bf46a2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsFNrhR_Wfb5"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
        "datasets = load_dataset('./messaging-chat-parser/data/chat_parsed')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#datasets = {'train': r_j_dataset, 'test':r_j_dataset}\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"longest\",  truncation=True)\n",
        "\n",
        "tokenized_datasets = datasets.map(tokenize_function, batched=True,remove_columns=[\"text\"])\n",
        "# block_size = tokenizer.model_max_length\n",
        "block_size = 128\n",
        "\n",
        "def group_texts(examples):\n",
        "    #print(examples.keys())\n",
        "    # Concatenate all texts.\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
        "        # customize this part to your needs.\n",
        "    total_length = (total_length // block_size) * block_size\n",
        "    # Split by chunks of max_len.\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy() #check predictions \n",
        "    return result\n",
        "    \n",
        "tokenized_datasets = tokenized_datasets.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    batch_size=1000,\n",
        "    num_proc=4,\n",
        ")\n"
      ],
      "metadata": {
        "id": "BVjVUgQ_ovpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiwZLb47X8HV"
      },
      "source": [
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(100)) \n",
        "small_eval_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(100)) \n",
        "full_train_dataset = tokenized_datasets[\"train\"]\n",
        "full_eval_dataset = tokenized_datasets[\"train\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwCyA5MMY4PD"
      },
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "777pcAuNZIue"
      },
      "source": [
        "from transformers import TrainingArguments\n",
        "torch.cuda.empty_cache()\n",
        "training_args = TrainingArguments(\"test_trainer\")\n",
        "#print(str(training_args))\n",
        "training_args.fp16 = True\n",
        "\n",
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBwemFaak1kV"
      },
      "source": [
        "chat = []\n",
        "start_temperature = 0.1\n",
        "max_temperature = 3.0\n",
        "\n",
        "for _ in range(5):\n",
        "    new_line = \"[others] \" + input(\"âœ\") + '\\n'\n",
        "    chat.append(new_line)\n",
        "    #ipdb.set_trace(context=6)\n",
        "    me_token = False\n",
        "    temperature = start_temperature\n",
        "    input_network = ' '.join(chat)\n",
        "    #ipdb.set_trace(context=6)\n",
        "    while not me_token:\n",
        "        text = ai.generate(prompt=input_network, \n",
        "                           return_as_list=True, \n",
        "                           temperature=temperature)\n",
        "        text = text[0] # batch of 1\n",
        "        #ipdb.set_trace(context=6)\n",
        "        text = text.split()\n",
        "        chat_pos = len(chat)\n",
        "        #ipdb.set_trace(context=6)\n",
        "        network_reply = text[chat_pos]\n",
        "        #ipdb.set_trace(context=6)\n",
        "        \n",
        "        if network_reply.startswith('[me]'):\n",
        "            me_token = True\n",
        "            network_reply = text[chat_pos] + '\\n'\n",
        "            chat.append(network_reply)\n",
        "        else:\n",
        "            if temperature >= max_temperature:\n",
        "                raise RuntimeError(\"Max temperature reached\")\n",
        "            temperature += 0.1\n",
        "            print(f'temperature exit: {temperature}')\n",
        "    print('Chat:')\n",
        "    print(chat)\n",
        "    print('---------------------')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}